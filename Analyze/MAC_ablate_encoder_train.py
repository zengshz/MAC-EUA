import time
import yaml
import os
import gc
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader
from Plan.MACAllocator_Ablate_Encoder import MACAllocator_Ablate_Encoder
from Monitoring.user_gen import gen_dataset
from Analyze.Utils import check_gradients

# è¯»å– config.yaml æ–‡ä»¶
with open('../config.yaml', 'r') as file:
    config = yaml.safe_load(file)

train_data_size = config.get('train_data_size')
miu = config.get('telecom_miu')
sigma = config.get('telecom_sigma')
radius_low = config.get('telecom_radius_low')
radius_high = config.get('telecom_radius_high')
batch_size = config.get('batch_large_size')
user_num = config.get('telecom_train_user_num')
server_percent = config.get('telecom_train_server_percent')
dataset_save_path = config.get('telecom_train_dataset_save_path')
server_path = config.get('server_telecom_path')
d_model = config.get('d_model')
dropout = config.get('dropout')
num_heads = config.get('num_heads')
edge_dim = config.get('edge_dim')
user_feature_dim = config.get('user_feature_dim')
server_feature_dim = config.get('server_feature_dim')
spatial_raw_dim = config.get('spatial_raw_dim')
server_state_dim = config.get('server_state_dim')
lr = config.get('lr')
patience = config.get('patience')
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
data_set = 'telecom'


def MAC_ablate_not_train():
    torch.autograd.set_detect_anomaly(False)

    # =======================
    # æ•°æ®åŠ è½½
    # =======================
    data_type = {'train': [], 'valid': []}
    dataset = gen_dataset(
        user_num, train_data_size, server_path, dataset_save_path, server_percent,
        radius_low, radius_high, miu, sigma, device, data_type, data_set
    )
    train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)
    valid_loader = DataLoader(dataset['valid'], batch_size=batch_size, shuffle=False)

    # =======================
    # æ¨¡å‹åˆå§‹åŒ–
    # =======================
    model = MACAllocator_Ablate_Encoder(d_model = d_model,
        num_heads = num_heads,
        dropout = dropout,
        edge_dim = edge_dim,
        user_feature_dim = user_feature_dim,
        server_feature_dim = server_feature_dim,
        spatial_raw_dim = spatial_raw_dim,
        server_state_dim = server_state_dim,
        device = device,
        MAX_PROPAGATION_LATENCY = radius_high,
        policy='sample').to(device)
    ema_model = MACAllocator_Ablate_Encoder(d_model = d_model,
        num_heads = num_heads,
        dropout = dropout,
        edge_dim = edge_dim,
        user_feature_dim = user_feature_dim,
        server_feature_dim = server_feature_dim,
        spatial_raw_dim = spatial_raw_dim,
        server_state_dim = server_state_dim,
        device = device,
        MAX_PROPAGATION_LATENCY = radius_high,
        policy='sample').to(device)
    # å°† model å½“å‰çš„æ‰€æœ‰å‚æ•° å®Œæ•´å¤åˆ¶ åˆ° ema_modelã€‚ è¿™æ ·ï¼Œema_model ä¸€å¼€å§‹ä¸ model çš„æƒé‡å®Œå…¨ä¸€è‡´ã€‚
    ema_model.load_state_dict(model.state_dict())
    for p in ema_model.parameters():
        p.requires_grad = False  # éå† ema_model çš„æ¯ä¸ª Parameterï¼Œç¦æ­¢æ¢¯åº¦è®¡ç®—ã€‚
    model.to(device)  # ç§»åˆ°ç›®æ ‡è®¾å¤‡
    ema_model.to(device)  # ç§»åˆ°ç›®æ ‡è®¾å¤‡

    def count_parameters(m):
        return sum(p.numel() for p in m.parameters() if p.requires_grad)

    print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°é‡: {count_parameters(model):,}")  # 267,521  990,977  210,465

    model_dir = f"./model/MACAllocator_Ablate_Encoder/{time.strftime('%m%d%H%M')}_server_{server_percent}_user_{user_num}_miu_{miu}_sigma_{sigma}"
    os.makedirs(model_dir, exist_ok=True)
    # AdamW æ˜¯ Adam çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä½¿æŸå¤±å‡½æ•°é€æ­¥æ”¶æ•›  weight_decay=1e-4 æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰
    # optimizer ä¼šåœ¨æ¯æ¬¡ loss.backward() è®¡ç®—æ¢¯åº¦åï¼Œé€šè¿‡ optimizer.step() æŒ‰ AdamW è§„åˆ™æ›´æ–° model.parameters()ã€‚
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    # ä½™å¼¦é€€ç« (Cosine Annealing) Warm Restartsï¼šå½“ä¸€ä¸ªå‘¨æœŸç»“æŸæ—¶ï¼Œå­¦ä¹ ç‡è·³å›åˆå§‹å€¼ï¼Œå¼€å§‹ä¸‹ä¸€è½®ä½™å¼¦ä¸‹é™ï¼Œç›¸å½“äºå¤šæ¬¡â€œé‡å¯â€è®­ç»ƒã€‚
    # T_0=10ï¼šç¬¬ä¸€æ¬¡ä½™å¼¦å‘¨æœŸçš„é•¿åº¦ T_mult=2ï¼šä¹‹åæ¯ä¸ªå‘¨æœŸçš„é•¿åº¦æ˜¯ä¸Šä¸€ä¸ªçš„ 2 å€ã€‚ä¾‹å¦‚å‘¨æœŸåºåˆ—æ˜¯ï¼š10ã€20ã€40ã€80
    # lr_scheduler.step()è¿™æ ·å­¦ä¹ ç‡å°±æŒ‰ç…§ä½™å¼¦æ›²çº¿ + å‘¨æœŸé‡å¯ç­–ç•¥æ›´æ–°ã€‚
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)

    # =======================
    # è®­ç»ƒå¾ªç¯
    # =======================
    epoch = 0
    best_val_loss = float('inf')
    patience = 25
    stagnation = 0
    # =======================
    # æ”¶æ•›é€Ÿåº¦è®°å½•ï¼ˆè®­ç»ƒæ›²çº¿ï¼‰
    # =======================
    curve = {
        "epoch": [],
        "train_loss": [],
        "val_loss": [],
        "user_ratio": [],
        "server_ratio": [],
        "capacity_ratio": [],
        "prop_lat": [],
        "lr": [],
        "time": []
    }
    start_time = time.time()

    while True:
        model.train()
        model.policy = 'sample'
        pbar = tqdm(train_loader, desc=f"[Epoch {epoch}] TRAIN")
        for srv, usr, conn, lat in pbar:
            srv, usr, conn, lat = map(lambda x: x.to(device), [srv, usr, conn, lat])
            # -----------------------
            # æ¸…ç©ºæ¢¯åº¦
            # -----------------------
            optimizer.zero_grad()

            loss, log_prob, alloc_num, user_ratio, active_ratio, cap_ratio, prop_lat = model(srv, usr, conn, lat)

            # -----------------------
            # è®¡ç®— Advantage (baseline)
            # -----------------------
            with torch.no_grad():
                V, *_ = ema_model(srv, usr, conn, lat)

            advantage = loss - V

            # -----------------------
            # Advantage æ ‡å‡†åŒ– (å‡å‡å€¼ / é™¤æ ‡å‡†å·®)
            # -----------------------
            adv_mean = advantage.mean()
            adv_std = advantage.std(unbiased=False) + 1e-6  # é˜²æ­¢é™¤é›¶
            advantage_normalized = (advantage - adv_mean) / adv_std

            # -----------------------
            # ç­–ç•¥æ¢¯åº¦æŸå¤±
            # -----------------------
            reinforce_loss = (advantage_normalized.detach() * log_prob).mean()

            # -----------------------
            # åå‘ä¼ æ’­
            # -----------------------
            reinforce_loss.backward()
            optimizer.step()

            # # ========== æ¢¯åº¦æ£€æŸ¥ ==========
            # # è®¡ç®—å…¨å±€æ¢¯åº¦èŒƒæ•°
            # total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1e6, norm_type=2)
            # # æ¢¯åº¦æ¶ˆå¤±æ£€æŸ¥ (èŒƒæ•°è¿‡å°)
            # if total_norm < 1e-3:
            #     print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°æ¢¯åº¦æ¶ˆå¤±ï¼Œå½“å‰æ¢¯åº¦èŒƒæ•° {total_norm:.4f} (æ‰¹æ¬¡ {epoch})")
            #     optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦ï¼Œé˜²æ­¢å¼‚å¸¸æ¢¯åº¦å½±å“è®­ç»ƒ
            #     continue
            # # æ¢¯åº¦çˆ†ç‚¸æ£€æŸ¥ (èŒƒæ•°è¿‡å¤§æˆ–å‡ºç°inf/NaN)
            # if total_norm > 1e6 or not torch.isfinite(total_norm):
            #     print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°æ¢¯åº¦çˆ†ç‚¸ï¼Œå½“å‰æ¢¯åº¦èŒƒæ•° {total_norm:.4f} (æ‰¹æ¬¡ {epoch})")
            #     optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦ï¼Œé˜²æ­¢å¼‚å¸¸æ¢¯åº¦å½±å“è®­ç»ƒ
            #     continue

            # -----------------------
            # å­¦ä¹ ç‡è°ƒåº¦
            # -----------------------
            lr_scheduler.step()

            # check_gradients(model)

            # -----------------------
            # EMA æ¨¡å‹æ›´æ–°
            # -----------------------
            decay = 0.99
            alpha = 1 - decay
            with torch.no_grad():
                for p_ema, p_model in zip(ema_model.parameters(), model.parameters()):
                    p_ema.mul_(decay).add_(p_model, alpha=alpha)

            # åªå–æ ‡é‡ï¼Œä¸ä¿ç•™ tensor
            loss_val = loss.mean().item()
            bl_val = V.mean().item()
            alloc_val = float(alloc_num.mean().item())
            ur = float(user_ratio.mean().item()) * 100.0
            svr = float(active_ratio.mean().item()) * 100.0
            cap = float(cap_ratio.mean().item()) * 100.0
            lat = float(prop_lat.mean().item()) * 100.0
            lr_val = optimizer.param_groups[0]['lr']

            pbar.set_postfix({
                "Actor": f"{loss_val:.4f}",
                "BL": f"{bl_val:.4f}",
                "AllocNum": f"{alloc_val:.1f}",
                "User%": f"{ur:.2f}%",
                "Svr%": f"{svr:.2f}%",
                "Cap%": f"{cap:.2f}%",
                "PropLat": f"{lat:.2f}",
                "LR": f"{lr_val:.6f}"
            })
            # -------- æ˜¾å¼é‡Šæ”¾ --------
            del loss, log_prob, alloc_num, user_ratio
            del active_ratio, cap_ratio, prop_lat
            del V, advantage, reinforce_loss

        # =======================
        # éªŒè¯
        # =======================
        model.eval()
        model.policy = 'greedy'
        val_losses, val_user_ratios, val_svr_ratios, val_cap_ratios, val_prop_lats = [], [], [], [], []

        with torch.no_grad():
            for srv, usr, conn, lat in valid_loader:
                srv, usr, conn, lat = map(lambda x: x.to(device), [srv, usr, conn, lat])
                val_loss, *_, val_user_ratio, val_svr_ratio, val_cap_ratio, val_prop_lat = model(srv, usr, conn, lat)
                val_losses.append(val_loss.mean().item())
                val_user_ratios.append(val_user_ratio.mean().item())
                val_svr_ratios.append(val_svr_ratio.mean().item())
                val_cap_ratios.append(val_cap_ratio.mean().item())
                val_prop_lats.append(val_prop_lat.mean().item())
        val_loss_mean = np.mean(val_losses)
        val_user_ratio_mean = np.mean(val_user_ratios)
        val_svr_ratio_mean = np.mean(val_svr_ratios)
        val_cap_ratio_mean = np.mean(val_cap_ratios)
        val_prop_lat_mean = np.mean(val_prop_lats)

        print(
            f"\n[VALID] Epoch {epoch} | BestLoss: {best_val_loss:.4f} | ValLoss: {val_loss_mean:.4f} | User%: {val_user_ratio_mean:.4%}"
            f" | Server%: {val_svr_ratio_mean:.2%} | Capacity%: {val_cap_ratio_mean:.2%} | Lat/m: {val_prop_lat_mean:.2%}")

        # =======================
        # æ¨¡å‹ä¿å­˜ & æ—©åœ
        # =======================
        if val_loss_mean < best_val_loss:
            best_val_loss = val_loss_mean
            stagnation = 0
            save_path = os.path.join(model_dir,
                                     f"{time.strftime('%m%d%H%M')}_{epoch}_alloc_{val_user_ratio_mean:.4f}_cap_{val_cap_ratio_mean:.4f}_lat_{val_prop_lat_mean:.4f}_best.pth")
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
        else:
            stagnation += 1
            print(f"âš ï¸ éªŒè¯æŒ‡æ ‡åœæ» {stagnation} è½®")
            if stagnation >= patience:
                print(f"â¹ï¸ è®­ç»ƒæ—©åœï¼Œ{patience} è½®æ— æå‡")
                break
        # # epoch ç»“æŸåå¯ä»¥æ˜¾å¼å›æ”¶
        torch.cuda.empty_cache()
        gc.collect()
        epoch += 1

        # =======================
        # æ”¶æ•›æ›²çº¿ï¼šè®°å½•æœ¬ epoch æ•°æ®
        # =======================
        curve["epoch"].append(epoch)
        curve["train_loss"].append(loss_val)  # æœ€åä¸€ä¸ª batch çš„è®­ç»ƒ loss
        curve["val_loss"].append(val_loss_mean.item())

        curve["user_ratio"].append(val_user_ratio_mean)
        curve["server_ratio"].append(val_svr_ratio_mean)
        curve["capacity_ratio"].append(val_cap_ratio_mean)
        curve["prop_lat"].append(val_prop_lat_mean)

        curve["lr"].append(lr_val)
        curve["time"].append(time.time() - start_time)

        # ä¿å­˜ä¸º numpyï¼Œä¾¿äºåç»­ç”»å›¾
        np.save(os.path.join(model_dir, "curve.npy"), curve)

    total_time = (time.time() - start_time) / 3600
    print(f"âœ… æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f} å°æ—¶")

if __name__ == '__main__':
    if torch.cuda.is_available():
        print(torch.__version__)  # éœ€è¦ >= 2.0.0
        # print(torch.cuda.is_available())  # åº”ä¸ºTrue
        # print(torch.cuda.get_device_capability())  # è®¡ç®—èƒ½åŠ›éœ€ >= (8,0)
        MAC_ablate_not_train()
    else:
        print("cuda.unavailable!")
