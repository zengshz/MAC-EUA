import torch
import numpy as np
import matplotlib.pyplot as plt



def calculate_propagation_latency(user_allocated: torch.LongTensor,
                                   distances: torch.Tensor) -> torch.Tensor:
    """
    åŸºäºåˆ†é…ç»“æœå’Œä¼ æ’­è·ç¦»çŸ©é˜µè®¡ç®—å¹³å‡ä¼ æ’­è·ç¦»ï¼ˆç±³ï¼‰ã€‚

    å‚æ•°ï¼š
        user_allocated - [B, U]ï¼Œç”¨æˆ·åˆ†é…åˆ°çš„æœåŠ¡å™¨ç´¢å¼•ï¼Œ-1 è¡¨ç¤ºæœªåˆ†é…
        distances      - [B, U, S]ï¼Œä¼ æ’­è·ç¦»çŸ©é˜µï¼Œè¡Œæ˜¯ç”¨æˆ·ï¼Œåˆ—æ˜¯æœåŠ¡å™¨ï¼ˆå•ä½ï¼šç±³ï¼‰

    è¿”å›ï¼š
        avg_distance   - [B]ï¼Œæ¯ä¸ª batch çš„å¹³å‡ä¼ æ’­è·ç¦»ï¼ˆç±³ï¼‰
    """
    B, U = user_allocated.shape

    # 1) æ©ç ï¼šå“ªäº›ç”¨æˆ·æ˜¯çœŸæ­£åˆ†é…åˆ°æœåŠ¡å™¨çš„
    mask = (user_allocated != -1)                        # [B, U]

    # 2) é˜²è¶Šç•Œç´¢å¼•
    idx = user_allocated.clone()
    idx[~mask] = 0                                       # å°†æœªåˆ†é…çš„ç´¢å¼•ç½®ä¸º0

    # 3) æ”¶é›†å¯¹åº”çš„ä¼ æ’­è·ç¦» [B, U]
    indices = idx.unsqueeze(-1)                          # [B, U, 1]
    prop_dist = torch.gather(distances, 2, indices).squeeze(-1)

    # 4) å°† nan æˆ– inf æ›¿æ¢æˆ 0ï¼Œç¡®ä¿åç»­è¿ç®—å®‰å…¨
    prop_dist = prop_dist.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)

    # 5) æŠŠæœªåˆ†é…çš„ä½ç½®å¼ºåˆ¶ä¸º 0
    prop_dist = prop_dist.masked_fill(~mask, 0.0)

    # 6) è®¡ç®—æ€»è·ç¦»ä¸å¹³å‡è·ç¦»
    total_dist   = prop_dist.sum(dim=1)                  # [B]
    valid_counts = mask.sum(dim=1).float()               # [B]

    # å¹³å‡ä¼ æ’­è·ç¦»ï¼Œé˜²æ­¢é™¤é›¶
    avg_distance = torch.zeros(B, device=distances.device)
    nonzero      = valid_counts > 0
    avg_distance[nonzero] = total_dist[nonzero] / valid_counts[nonzero]
    return avg_distance


def check_gradients(model, plot_grad_dist=True):
    """
    æ£€æŸ¥æ¨¡å‹å‚æ•°çš„æ¢¯åº¦çŠ¶æ€å¹¶å¯è§†åŒ–æ¢¯åº¦åˆ†å¸ƒ

    å‚æ•°:
    model: nn.Module
        è¦æ£€æŸ¥çš„PyTorchæ¨¡å‹
    plot_grad_dist: bool (é»˜è®¤ä¸ºTrue)
        æ˜¯å¦ç»˜åˆ¶æ¢¯åº¦åˆ†å¸ƒç›´æ–¹å›¾
    """
    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
    for name, param in model.named_parameters():
        if param.grad is None:
            print(f"âš ï¸ [æ¢¯åº¦ä¸¢å¤±] {name}: æœªæ¥æ”¶æ¢¯åº¦")
        else:
            grad_norm = param.grad.norm().item()
            print(f"âœ… [æ¢¯åº¦æ­£å¸¸] {name}: æ¢¯åº¦èŒƒæ•°={grad_norm:.4e}")

    # å¯è§†åŒ–æ¢¯åº¦åˆ†å¸ƒ
    if plot_grad_dist:
        all_grads = []
        for param in model.parameters():
            if param.grad is not None:
                # å°†æ¢¯åº¦æ•°æ®è½¬ä¸ºnumpyæ•°ç»„å¹¶å±•å¹³
                all_grads.append(param.grad.detach().view(-1).cpu().numpy())

        if len(all_grads) > 0:
            all_grads = np.concatenate(all_grads)
            plt.figure(figsize=(10, 6))
            plt.hist(all_grads, bins=100, alpha=0.7)
            plt.yscale('log')
            plt.xlabel("Gradient Value")
            plt.ylabel("Frequency (log scale)")
            plt.title("Gradient Distribution")
            plt.grid(True, which="both", ls="--")
            # **ä¿å­˜ä¸ºå›¾ç‰‡**
            plt.savefig("gradient_distribution.png")  # é¿å… plt.show() çš„é—®é¢˜
            print("ğŸ“Š æ¢¯åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜ä¸º gradient_distribution.png")
        else:
            print("âš ï¸ æ‰€æœ‰å‚æ•°å‡æ— æ¢¯åº¦ï¼Œæ— æ³•ç»˜åˆ¶åˆ†å¸ƒå›¾")
